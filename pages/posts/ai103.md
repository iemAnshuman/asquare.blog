---
title: Artificial Intelligence 103
date: 2025-09-21
type: note
---

### Part 1: Datasets

At its heart, machine learning is about finding patterns in data. The data itself is organized into a **dataset**, which is typically a table of examples.

- Each row represents a **sample** (also called an example, instance, or data point).
- Each column represents a **feature** (also called an attribute or variable).

#### Feature Sets and Labels

A typical dataset for supervised learning is split into two parts:

- **Features:** These are the input variables, the pieces of information we have. In a dataset to predict house prices, the features might be `Square_Footage`, `Num_Bedrooms`, and `Year_Built`. The collection of all features for a single sample is its **feature set** or **feature vector**.
- **Label:** This is the output variable, the thing we are trying to predict. For the housing dataset, the label would be `Price`.

The fundamental goal of a supervised ML model is to learn the relationship—the hidden pattern—that connects the feature set to the label.

---

### Preparing Data for Learning - Preprocessing

A common saying in machine learning is **"Garbage in, garbage out."** A sophisticated algorithm fed with poor-quality data will produce poor results. That's why a huge amount of a data scientist's time is spent on preparing and cleaning the dataset. This involves several key steps:

- **Handling Missing Values:** Real-world datasets often have holes. A model can't handle a missing value, so we must decide on a strategy, such as filling it with the mean or median of the column (imputation) or dropping the row entirely.
- **Feature Engineering:** This is the creative process of creating new, more powerful features from existing ones. For example, from `Num_Accidents` and `Years_Driving`, we could engineer a new feature called `Accidents_Per_Year`, which might be more predictive.
- **Feature Scaling:** Many ML algorithms are sensitive to the scale of the features. If one feature ranges from 0-1 and another from 0-100,000, the algorithm might incorrectly assume the second feature is more important. We fix this by scaling the data, typically with one of two methods:
  - **Normalization (Min-Max Scaling):** Rescales the data to a fixed range, usually 0 to 1. The formula for a feature value $x$ is:
    $$x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$$
  - **Standardization (Z-score):** Rescales the data to have a mean ($μ$) of 0 and a standard deviation ($σ$) of 1. The formula is:
    $$x_{std} = \frac{x - \mu}{\sigma}$$

---

### Part 3: How a Model Generalizes - Dataset Division

Let's say we have our clean dataset. The goal is to train a model that performs well not just on the data we have, but on new, unseen data. This is the concept of **generalization**.

#### The Enemy: Overfitting

The biggest obstacle to generalization is **overfitting**. This happens when a model learns the training data _too_ well. It memorizes not only the underlying patterns but also the noise and random fluctuations specific to that data.

Think of a student who memorizes the exact questions and answers from a practice exam. They'll ace that practice test, but when they see slightly different questions on the real exam, they will fail because they never learned the underlying concepts. An overfitted model is just like that.

#### The Solution: Splitting the Dataset

To prevent overfitting and to properly evaluate our model, we never train on our entire dataset. We split it into three distinct sets:

1.  **Training Set (60-80%):** This is the data the model actually "sees" and learns from. The model adjusts its internal parameters based on the patterns in this set.
2.  **Validation Set (10-20%):** After training, we use this set to see how the model performs on data it hasn't seen before. We use this set to tune the model's **hyperparameters**—the external settings of the model, like its complexity or learning rate. We can tweak the model and re-evaluate on the validation set until we are happy with its performance.
3.  **Test Set (10-20%):** This set is the final exam. We keep it locked away and only use it _once_, at the very end, after all training and tuning is complete. The performance on the test set gives us an unbiased estimate of how our model will perform in the real world on completely new data.

#### The Gold Standard: Cross-Validation

A single train-validation split can be subject to luck. Maybe we were just lucky (or unlucky) with the specific data points that ended up in our validation set. A more robust technique is **k-fold Cross-Validation**.

Here's how it works:

1.  We split our data (minus the test set) into `k` equal-sized parts, or "folds" (e.g., 5 or 10).
2.  We perform `k` rounds of training. In each round, we use one fold as the validation set and the remaining `k-1` folds as the training set.
3.  We average the performance scores from all `k` rounds.

This gives us a much more reliable estimate of our model's performance and reduces the impact of random chance in the split.

---

### Part 4: Taming Complexity - Dimensionality Reduction

Sometimes, our datasets have too many features. While more data is often good, too many features (or dimensions) can lead to the **Curse of Dimensionality**. In very high-dimensional spaces, data points become sparse and far apart, making it harder for a model to find patterns. It also increases computational costs and the risk of overfitting.

**Dimensionality Reduction** techniques aim to reduce the number of features while retaining as much useful information as possible.

#### Principal Component Analysis (PCA)

PCA is an **unsupervised** technique that transforms the data into a new set of features, called **principal components**. It works by finding the directions of maximum **variance** in the data. The first principal component is the direction that captures the most variation, the second captures the next most (and is perpendicular to the first), and so on. We can then keep only the first few components, effectively reducing the dimensionality while keeping the most important information.

**Analogy:** Imagine trying to capture the essence of a 3D car model in a 2D photograph. You wouldn't take the picture from the top or bottom. You'd find the angle (like a side view) that shows the most defining features. PCA is like finding that best angle for your data.

#### Linear Discriminant Analysis (LDA)

LDA is a **supervised** technique, meaning it uses the class labels. While PCA tries to find the directions of maximum variance, LDA tries to find the directions that **maximize the separability between classes**. It projects the data onto a lower-dimensional space in a way that pushes different classes as far apart as possible.

**Analogy:** Imagine you have red and blue marbles scattered in a 3D box. PCA would find the axis along which the marbles are most spread out, regardless of color. LDA would find the axis that, when you look along it, shows the clearest separation between the cluster of red marbles and the cluster of blue marbles.

#### Independent Component Analysis (ICA)

ICA is another **unsupervised** technique with a different goal. It assumes that your observed features are a mixture of some underlying, independent sources. ICA tries to "unmix" them.

**Analogy:** This is famously known as the **"cocktail party problem."** Imagine you are in a room with two people talking, and you have two microphones placed at different spots. Each microphone records a mixture of both voices. ICA is the algorithm that can take those two mixed recordings and separate them back into two clean recordings, one for each speaker. In data analysis, it's used to find underlying hidden factors that are statistically independent.
