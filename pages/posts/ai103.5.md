---
title: Artificial Intelligence 103.5
date: 2025-09-21
type: note
---

### Part 1: The Dataset - Agricultural Pest Detection

Imagine we're building a system for a smart farming drone. The drone's camera measures physical characteristics of insects it finds, and our goal is to classify them as either a "Beneficial Insect" (like a bee, label `0`) or a "Harmful Pest" (like a locust, label `1`).

Here is our raw data for six insects the drone has scanned:

| SampleID | Size (mm) | Weight (mg) | Wing_Span (mm) | Type (Label)   |
| :------- | :-------- | :---------- | :------------- | :------------- |
| 1        | 10        | 150         | 22             | 1 (Harmful)    |
| 2        | 5         | 50          | 10             | 0 (Beneficial) |
| 3        | 12        | 160         | 25             | 1 (Harmful)    |
| 4        | 7         | 80          | 15             | 0 (Beneficial) |
| 5        | 15        | 200         | 30             | 1 (Harmful)    |
| 6        | 6         | 65          | 12             | 0 (Beneficial) |

Looking at this raw data, we can immediately spot a problem. The `Weight (mg)` feature has values that are roughly 10-20 times larger than the `Size (mm)` feature. An algorithm that uses distance calculations (like k-Nearest Neighbors) would be completely dominated by the `Weight` feature, practically ignoring the others. We need to fix this.

---

### Part 2: Data Preprocessing - Standardization in Action

To solve the scaling issue, we'll apply **Standardization (Z-score Scaling)**, which will give each feature a mean of 0 and a standard deviation of 1.

The formula is: $x_{std} = \frac{x - \mu}{\sigma}$

**Step 1: Calculate Mean ($μ$) and Standard Deviation ($σ$) for each feature.**

- **Size (mm):**
  - $μ_{size} = (10+5+12+7+15+6) / 6 = 9.17$
  - $σ_{size} = 3.69$
- **Weight (mg):**
  - $μ_{weight} = (150+50+160+80+200+65) / 6 = 117.5$
  - $σ_{weight} = 59.4$
- **Wing_Span (mm):**
  - $μ_{span} = (22+10+25+15+30+12) / 6 = 19.0$
  - $σ_{span} = 7.4$

**Step 2: Apply the formula to each data point.**

Let's calculate the standardized value for Sample 1 (`Size = 10`):
$x_{std} = (10 - 9.17) / 3.69 = 0.22$

After doing this for every value, our dataset is transformed:

| SampleID | Size (std) | Weight (std) | Wing_Span (std) | Type (Label) |
| :------- | :--------- | :----------- | :-------------- | :----------- |
| 1        | 0.22       | 0.55         | 0.41            | 1            |
| 2        | -1.13      | -1.14        | -1.22           | 0            |
| 3        | 0.77       | 0.72         | 0.81            | 1            |
| 4        | -0.59      | -0.63        | -0.54           | 0            |
| 5        | 1.58       | 1.39         | 1.49            | 1            |
| 6        | -0.86      | -0.88        | -0.95           | 0            |

Now all features are on the same scale, and no single feature will unfairly dominate the learning process.

---

### Part 3: Evaluation Strategy - 3-Fold Cross-Validation

With our data preprocessed, how do we evaluate a model trained on it? A simple train-test split is risky with a small dataset. Instead, we'll use **3-fold Cross-Validation**.

First, we split our 6-sample dataset into 3 "folds":

- **Fold 1:** {Sample 1, Sample 2}
- **Fold 2:** {Sample 3, Sample 4}
- **Fold 3:** {Sample 5, Sample 6}

Now we perform three rounds of training and validation:

- **Round 1:**

  - **Train on:** Folds 2 & 3 ({S3, S4, S5, S6})
  - **Validate on:** Fold 1 ({S1, S2})
  - Let's say our model predicts correctly, **Validation Accuracy = 100%**.

- **Round 2:**

  - **Train on:** Folds 1 & 3 ({S1, S2, S5, S6})
  - **Validate on:** Fold 2 ({S3, S4})
  - Let's say it predicts S3 correctly but S4 incorrectly, **Validation Accuracy = 50%**.

- **Round 3:**
  - **Train on:** Folds 1 & 2 ({S1, S2, S3, S4})
  - **Validate on:** Fold 3 ({S5, S6})
  - Let's say it predicts correctly, **Validation Accuracy = 100%**.

**Final Result:** The cross-validated accuracy is the average of the scores from each round:
$$Final\;Accuracy = \frac{100\% + 50\% + 100\%}{3} = 83.3\%$$
This gives us a much more reliable estimate of the model's performance than a single split.

---

### Part 4: Dimensionality Reduction - Applying PCA

Our dataset has three features. For visualization and efficiency, let's see if we can reduce it to two dimensions using **Principal Component Analysis (PCA)**. We won't do the full matrix algebra here, but we'll walk through the conceptual steps.

1.  **Start with Standardized Data:** PCA is sensitive to scale, so this step is mandatory. We'll use our standardized table from Part 2.
2.  **Compute Covariance Matrix:** We calculate a 3x3 covariance matrix that shows how the features (`Size`, `Weight`, `Wing_Span`) vary with each other. We notice that they are all highly correlated—bigger insects tend to be heavier and have larger wingspans.
3.  **Calculate Eigenvectors & Eigenvalues:** We find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the new dimensions (the Principal Components), and the eigenvalues tell us how much variance each new dimension captures.
4.  **Select Components:** Let's say the analysis gives us these (hypothetical) eigenvalues:

    - Eigenvalue for PC1: **2.75**
    - Eigenvalue for PC2: **0.20**
    - Eigenvalue for PC3: **0.05**
    - **Total Variance** (sum of eigenvalues) = 2.75 + 0.20 + 0.05 = 3.0
    - **Variance explained by PC1:** $2.75 / 3.0 = 91.7\%$
    - **Variance explained by PC2:** $0.20 / 3.0 = 6.7\%$

    Together, the first two principal components capture $91.7\% + 6.7\% = 98.4\%$ of the total variance in the data! We can safely discard the third component.

5.  **Transform Data:** We project our 3D standardized data onto the 2D subspace defined by PC1 and PC2. The result is a brand new dataset:

| SampleID | Principal Component 1 | Principal Component 2 | Type (Label) |
| :------- | :-------------------- | :-------------------- | :----------- |
| 1        | 0.65                  | -0.10                 | 1            |
| 2        | -1.82                 | 0.05                  | 0            |
| 3        | 1.15                  | -0.08                 | 1            |
| 4        | -0.92                 | 0.04                  | 0            |
| 5        | 2.49                  | 0.15                  | 1            |
| 6        | -1.54                 | -0.06                 | 0            |

We now have a 2D dataset that is much easier to visualize and can lead to faster model training, all while retaining 98.4% of the original information.

### Conclusion: Ready for Learning

We have successfully taken a raw dataset and put it through a rigorous data preparation pipeline. We:

1.  **Cleaned** it by standardizing the features.
2.  Devised a **robust evaluation plan** with cross-validation.
3.  **Engineered** a more efficient set of features using PCA.

This processed data is now in the ideal state to be fed into a machine learning algorithm. We've laid the solid groundwork required to build a model that is not only accurate but also reliable and efficient. In our next session, we'll finally take this data and train a classification model.
